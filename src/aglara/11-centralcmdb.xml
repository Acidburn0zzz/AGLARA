<?xml version="1.0" encoding="UTF-8"?>
<chapter version="5.0" xml:id="centralcmdb"
         xmlns="http://docbook.org/ns/docbook"
         xmlns:xlink="http://www.w3.org/1999/xlink"
         xmlns:xi="http://www.w3.org/2001/XInclude"
         xmlns:svg="http://www.w3.org/2000/svg"
         xmlns:m="http://www.w3.org/1998/Math/MathML"
         xmlns:html="http://www.w3.org/1999/xhtml"
         xmlns:db="http://docbook.org/ns/docbook">
  <title>Configuration management with git and Puppet</title>

  <section>
    <title>Introduction</title>

    <para>When working in a larger environment, using a central configuration
    management environment allows you to quickly rebuild vital systems or
    create copies, as well as keep track of your inventory and services. By
    centralizing all information, decisions and impact analysis can be done
    quickly and efficiently. In this chapter, we will focus on using Puppet
    within the reference architecture.</para>

    <warning>
      <para>TODO only use non-managed mode.</para>

      <para>Servers pull in data (git pull) on scheduled times (job
      scheduler).</para>

      <para>Content teams provide their modules through a puppet module
      server, which admin teams use. Admin teams use a single repository (or
      multiple if you want, then the servers will pull from "their"
      repository). As such, no need to combine systems.</para>

      <para>Technologies include gitosis &amp; puppet</para>

      <para>Perhaps use TXT record for repository source per server?</para>
    </warning>

    <section>
      <title>Central configuration management, or federated?</title>

      <para>Configuration management in a larger environment is not something
      that is easy to implement. It is a combination of deployment
      information, instance configuration management, historical information,
      parameter relations and more. In this reference architecture, the
      approach in the next figure is used.</para>

      <figure>
        <title>Overview for configuration management</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="images/09-cmdb-overview.png" scalefit="0"
                       width="100%"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>In this example, one team could be responsible for JBoss
      application server components (packages, documentation, third line
      support) but not have the responsibility for installing and managing
      JBoss AS. In this situation, this team delivers and manages their
      components, including an overlay (either for the team or shared with
      others). The system administrators configure their systems to add this
      particular overlay as well as configure their systems to properly set up
      JBoss AS by instantiating the modules developed by the first team on the
      systems.</para>

      <section>
        <title>Federated repositories</title>

        <para>First of all, the reference architecture uses a federated
        repository approach. Teams with their own responsibility use their own
        repositories (and managed in the way that is most efficient for those
        teams). On the figure, there are two "types" of teams represented, but
        this is only an example:</para>

        <itemizedlist>
          <listitem>
            <para>development teams, who are responsible for providing the
            necessary packages, documentation and more. In case of a
            Gentoo-only installation environment, those teams will manage
            overlays (structured sets of ebuilds) for their components.</para>
          </listitem>

          <listitem>
            <para>infrastructural teams, who are responsible for the
            infrastructure servers themselves. These teams manage their
            servers in their own repositories, which are checkout out on the
            configuration management hubs. With the use of proper branch
            names, the configuration management hubs can checkout testing
            branches and production branches for use on the target
            systems.</para>
          </listitem>
        </itemizedlist>

        <para>The use of federated repositories allows each team to work on
        their components in the most flexible manner. It also allows a
        reasonable access control on the various components: team 4 might not
        be granted write access on the system configuration for team 3 but can
        still read it (for its own internal testing), or perhaps even not read
        it at all.</para>
      </section>

      <section>
        <title>High-available hubs</title>

        <para>The configuration management HUBs are set up in a high-available
        manner. Because they only contain checkouts of the repositories (and
        do not act as the master repositories themselves) we can easily scale
        this architecture. In the picture it is shown as a multi-master (i.e.
        each HUB manages servers), load-balanced setup. However, other
        architectures can easily be implemented, such as a HUB for one site
        (data center) and a HUB for another site (another data center).</para>

        <para>The hubs contain the CMDB information needed for deployment. In
        our reference architecture, this will be Puppet.</para>
      </section>

      <section>
        <title>Version controlled or not</title>

        <para>In many organizations, changes on systems should be version
        controlled. By using version control repositories (such as Git) this
        is implemented automatically, but raises another question: how can
        teams fix a particular setting for one environment? This requirement
        has to be taken up with the specific solutions (such as Puppet and
        Git).</para>
      </section>
    </section>

    <section>
      <title>About Puppet</title>

      <para>Puppet is a free software configuration management tool, written
      in Ruby and developed under the wings of the Puppet Labs company.</para>

      <para>The idea behind Puppet is that administrators describe how a
      system should look like, and Puppet will devise the strategy to
      configure the system up to that point. This removes the cludge of
      re-inventing the order of actions as this is handled by Puppet. The
      declarative state of a system can also be applied to other systems
      easily, making it a breeze to create copies of existing systems.</para>

      <section>
        <title>Puppet definition structure</title>

        <para>The strength of Puppet lays within its definitions. You have
        modules that help you define the state of a particular resource (or
        set of resources), including default values that fit your needs, and
        you have nodes that instantiate the modules and set specifics. The
        power lays in the inheritance and overrides that you can define. In
        this architecture, let's consider the following definition
        structure:</para>

        <programlisting>/etc/puppet/
+- manifests
|  +- site.pp  # Global definition 
|  +- nodes.pp # Sources in the node definitions further down
|  +- nodes
|  |  +- team1
|  |  |  +- nodes.pp
|  |  |  `- ...
|  |  `- teamN
|  `- patterns
|     +- pattern1
|     `- patternN
+- modules
|  +- module1
|  `- moduleN
`- environments
   +- manifests
   |  `- nodes
   +- environment1
   |  +- modules
   |  `- patterns
   `- environmentN</programlisting>

        <para>System administration teams have their repository in which they
        define the state of their systems. This repository is available on the
        Puppet master site under the
        <filename>/etc/puppet/manifests/nodes/&lt;repo&gt;</filename>
        location. Each repository contains a nodes.pp file that is sourced in
        from the main <filename>nodes.pp</filename> at
        <filename>/etc/puppet/manifests</filename>. Nodes inherit or include
        information from patterns (and perhaps modules, but mostly
        patterns).</para>

        <para>Modules are developed independently as their own repositories,
        and made available under
        <filename>/etc/puppet/modules/&lt;repo&gt;</filename>. Modules are
        seen as building blocks of a particular technology and should have as
        little dependencies as possible.</para>

        <para>Patterns are also a sort-of modules, but they combine the
        previous modules in well structured classes. For instance, a pattern
        for a postgresql database server will include the postgresql module,
        but also a backup module (for instance for bacula), log rotation
        information, additional users (accounts for third line support),
        etc.</para>

        <para>Finally, environments are specific (tagged) checkouts of the
        modules and patterns, and are used to provide a release-based
        approach. Whereas the main location could contain the master branch of
        all repositories (which is the production-ready branch) the
        environments can support preproduction checkouts (for instance an
        environment called "development" and one called "testing") or even
        post-production checkouts (specific releases). If your organization
        uses monthly releases, this could be an environment production-201212
        denoting the december release in 2012. However, know that the
        environments also require the node definitions again and that it is
        the Puppet <emphasis>agent</emphasis> that defines which environment
        to use.</para>
      </section>

      <section>
        <title>Alternative: Using packaged releases</title>

        <para>If you do not like the use of environments as depicted above,
        you can also focus on packaged releases: teams still manage their code
        as they please, but eventually create packages (like ebuilds) which
        are versioned and can be deployed. This would then lead to packages
        that install modules with have a version in their name:</para>

        <programlisting>/etc/puppet
`- modules
   +- postgresql92-1.0.0
   `- postgresql92-1.0.2</programlisting>

        <para>Other packages (like those providing the patterns) then depend
        on the correct versions. Thanks to dependency support in Gentoo, when
        no patterns (or nodes) are installed anymore that depend on a
        particular version, it will be cleaned from the environment. And
        thanks to SLOT support, multiple deployments of the same package with
        different versions is supported as well.</para>

        <para>In this reference architecture, I will not persue this method -
        I find it a broken solution for a broken situation: in your
        architecture, fixing versions leads to outdated systems and slowly
        progressing information architecture. By leveraging environments, this
        problem is less prominent.</para>
      </section>
    </section>

    <section>
      <title>About Git</title>

      <para>Git is a distributed revision control system where developers have
      a full repository on their workstation, pushing changes to a remote
      origin location where others can then pull the changes from.</para>

      <para>The use of Git is popular in the free software world as it has
      proven itself fruitful for distributed development (such as with the
      Linux kernel), which is perfect in our reference architecture.</para>
    </section>
  </section>

  <section>
    <title>Git with gitolite</title>

    <para>Git by itself is a fairly modest versioning system. It doesn't
    support access controls out-of-the-box, instead relying on the abilities
    of other systems (such as those used to provide access to git, like the
    access control systems of the Linux operating system) to provide this
    important security feature.</para>

    <para>To make access controls easier to implement, as well as the
    management of the git repositories, various software titles have emerged.
    In this section, I'll focus on the gitolite software, which provides
    internal access controls (without needing to provide accounts on Linux
    operating system level) with SSH keys.</para>

    <section>
      <title>Architecture</title>

      <para>Gitolite<indexterm>
          <primary>gitolite</primary>
        </indexterm> uses a single operating system user, and abstracts access
      towards the git repositories it manages. Access towards git is done
      through SSH.</para>

      <para>Because git provides every developer with its own, full
      repository, we will not setup a high available architecture for git.
      Instead, we'll rely on its distributed model (and of course frequent
      backups).</para>

      <section>
        <title>Flows</title>

        <para>Only one flow is identified, which is the backup.</para>

        <figure>
          <title>Git and gitolite flows</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="images/09-git-flow.png" scalefit="0"
                         width="100%"/>
            </imageobject>
          </mediaobject>
        </figure>
      </section>

      <section>
        <title>Administration</title>

        <para>Gitolite itself is managed through a git repository of its own.
        Only when things are awkwardly failing on that level as well will you
        need SSH access to the server.</para>

        <figure>
          <title>Gitolite administration</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="images/09-git-admin.png" scalefit="0"
                         width="100%"/>
            </imageobject>
          </mediaobject>
        </figure>
      </section>

      <section>
        <title>Monitoring</title>

        <para>To monitor git, we'll use a testing repository on the system and
        a remote git commit. This allows us to make sure that the service is
        running and available, and might even be updated with some performance
        metrics (for instance, the time that a push of a small change
        takes).</para>
      </section>

      <section>
        <title>Operations</title>

        <para>Regular operations is the same as with the administration: users
        connect to the git server through git (and SSH).</para>
      </section>

      <section>
        <title>Users</title>

        <para>Users are managed through the gitolite repository.</para>
      </section>

      <section>
        <title>Security</title>

        <para>The secure access towards the repositories is handled by
        gitolite (through the configuration in the gitolite administration
        repository) and SSH.</para>
      </section>
    </section>

    <section>
      <title>Using gitolite</title>

      <para/>

      <section>
        <title>Installing &amp; configuring gitolite</title>

        <para>Installing gitolite in Gentoo is a breeze (just like with the
        majority of other distributions).</para>

        <programlisting># <command>emerge dev-vcs/gitolite</command></programlisting>

        <para>Next, copy the SSH key of the administrator(s) somewhere on the
        system, and then have the following command ran as the git user (you
        can use "<command>su -u git -c</command>" or "<command>sudo -H -u
        git</command>" for this), where
        <filename>/path/to/admin.pub</filename> is the public SSH key of the
        administrator:</para>

        <programlisting>git $ <command>gl-setup /path/to/admin.pub</command></programlisting>

        <para>The configuration file of gitolite will be shown. In it, you can
        configure gitolite.</para>

        <para>For instance:</para>

        <itemizedlist>
          <listitem>
            <para>set <varname>$REPO_UMASK</varname> to 0027 to ensure the
            repositories are not readable (and especially not writeable) by
            other users on the git server</para>
          </listitem>
        </itemizedlist>

        <para>Finally, have the administrator check out the gitolite
        administrative repository on his (client) system.</para>

        <programlisting>$ <command>git clone git@git.internal.genfic.com:gitolite-admin.git</command></programlisting>
      </section>

      <section>
        <title>Managing users</title>

        <para>Users are managed through their public SSH keys. Once you
        obtained a public SSH key for a user, commit it into the keydir/
        location (inside the gitolite-admin repository) named after the user
        (like <filename>keydir/username.pub</filename>). The filename itself
        is used to identify the users. If a user needs to be removed, remove
        the key from the directory and push the changes to the administration
        repository. The changes will take effect immediately.</para>
      </section>

      <section>
        <title>Managing repositories</title>

        <para>To create a new repository, edit the conf/gitolite.conf file and
        add in (or remove) the repository, and identify the user or users that
        are allowed access to the repository. If you remove a repository
        configuration, you'll need to remove the repository from the Linux
        host itself as well (which isn't done through the gitolite-admin
        repository).</para>

        <para>For instance, a snippet for a repository "puppet-was" in which
        the users john, dalia (both admins), jacob and eden have access
        to:</para>

        <programlisting>repo puppet-was
  RW+ = john dalia
  RW  = jacob
  R   = eden</programlisting>

        <para>The gitolite documentation referred to at the end of this
        chapter has more information about the syntax and the abilities,
        including group support in gitolite.</para>
      </section>
    </section>
  </section>

  <section>
    <title>Puppet</title>

    <para>The puppet master hosts the configuration entries for your
    environment and manages the puppet clients' authentication (through
    certificates).</para>

    <section>
      <title>Architecture</title>

      <para>The puppet architecture is fairly simple, which is also one of its
      strengths.</para>

      <section>
        <title>Flows</title>

        <para>The following diagram shows the flows/feeds that interact with
        the puppet processes.</para>

        <figure>
          <title>Flows towards and from puppet</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="images/09-puppet-flow.png" scalefit="0"
                         width="100%"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para>The most prominent flow is the one with configuration updates.
        These updates come from one of the Git repositories and are triggered
        locally on the puppet master server itself.</para>
      </section>

      <section>
        <title>Administration</title>

        <para>As puppet is an administration tool by itself, it comes to no
        surprise that the actual administration on puppet is done using the
        system-specific interactive shells (i.e. through SSH).</para>

        <figure>
          <title>Puppet administration</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="images/09-puppet-admin.png" scalefit="0"
                         width="100%"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para>The main administration task in puppet is handling the
        certificates: system administrators request a certificate through the
        puppet client. The client connects to the master, sends the signing
        request, which is then queued. The puppet admin then lists the pending
        certificate requests and signs those he know are valid. When signed,
        the system administrator can then retrieve the signed certificate and
        have it installed on the system (again through the puppet client) from
        which point the system is known and can be managed by puppet.</para>
      </section>

      <section>
        <title>Monitoring</title>

        <para>When checking on the puppet operations, we need to make sure
        that</para>

        <itemizedlist>
          <listitem>
            <para>the puppet agent is running (or scheduled to run)</para>
          </listitem>

          <listitem>
            <para>the puppet agent ran within the last xx minutes (depending
            on the frequency of the data gathering)</para>
          </listitem>

          <listitem>
            <para>the puppet agent did not fail</para>
          </listitem>
        </itemizedlist>

        <para>We might also want to include a check that says that n
        consecutive polls might not perform changes every time (in other
        words, the configuration has to be stable after n-1 requests).</para>
      </section>

      <section>
        <title>Operations</title>

        <para>During regular operations, the puppet agent frequently connects
        to the puppet master, sends all his "facts" (the state of the current
        system) from which the puppetmaster then devises how to update the
        system to match the configuration the system should be in.</para>

        <figure>
          <title>Regular operations of puppet</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="images/09-puppet-operations.png"
                         scalefit="0" width="100%"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para>The activities are, by default, triggered from the agent. It is
        possible (and we will do so later) to configure the agent to also
        listen for incoming connections from the puppet master. This allows
        administrators to push changes to systems without waiting for the
        agents to connect to the master.</para>
      </section>

      <section>
        <title>User management</title>

        <para>Puppet does not have specific user management features in it. If
        you want separate roles, you will need to do so through the file
        access control mechanisms on the puppet master and/or through the
        repositories that you use as the configuration repository.</para>
      </section>

      <section>
        <title>Security</title>

        <para>Make sure that no resources can be accessed through Puppet that
        are otherwise not accessible by unauthorized people. As Puppet
        includes a (web-based) file server, we need to configure it properly
        so that unauthorized access is not possible. Luckily, this is the
        default behavior with a Puppet installation.</para>
      </section>
    </section>

    <section>
      <title>Setting up puppet master</title>

      <para/>

      <section>
        <title>Installing puppet master</title>

        <para>The puppet master and puppet client itself are both provided
        through the <package>app-admin/puppet</package> package.</para>

        <programlisting># <command>equery u puppet</command>
[ Legend : U - final flag setting for installation]
[        : I - package is installed with flag     ]
[ Colors : set, unset                             ]
 * Found these USE flags for app-admin/puppet-2.7.18:
 U I
 + + augeas              : Enable augeas support
 - - diff                : Enable diff support
 - - doc                 : Adds extra documentation (API, Javadoc, etc). It is \
                           recommended to enable per package instead of globally
 - - emacs               : Adds support for GNU Emacs
 - - ldap                : Adds LDAP support (Lightweight Directory Access Protocol)
 - - minimal             : Install a very minimal build (disables, for example, \
                           plugins, fonts, most drivers, non-critical features)
 - - rrdtool             : Enable rrdtool support
 + + ruby_targets_ruby18 : Build with MRI Ruby 1.8.x
 - - shadow              : Enable shadow support
 - - sqlite3             : Adds support for sqlite3 - embedded sql database
 - - test                : Workaround to pull in packages needed to run with \
                           FEATURES=test. Portage-2.1.2 handles this internally, \
                           so don't set it in make.conf/package.use anymore
 - - vim-syntax          : Pulls in related vim syntax scripts
 - - xemacs              : Add support for XEmacs

# <command>emerge app-admin/puppet</command></programlisting>

        <para>Next, edit <filename>/etc/puppet/puppet.conf</filename> and add
        the following to enable puppetmaster to bind on IPv6:</para>

        <programlisting>[master]
    bindaddress="::"</programlisting>

        <para>You can then start the puppet master service.</para>

        <programlisting># <command>run_init rc-service puppetmaster start</command></programlisting>
      </section>

      <section>
        <title>Configuring as CA</title>

        <para>One puppet master needs to be configured as the certificate
        authority, responsible for handing out and managing the certificates
        of the various puppet clients.</para>

        <programlisting># cat /etc/puppet/puppet.conf
[main]
  logdir=/var/log/puppet
  rundir=/var/run/puppet
  ssldir=$vardir/ssl
[master]
  bindaddress="::"
  </programlisting>
      </section>

      <section>
        <title>Configuring as (non-CA) Hub</title>

        <para>The remainder of puppet masters need to be configured as a HUB;
        for these systems, disable CA functionality:</para>

        <programlisting># cat /etc/puppet/puppet.conf
[main]
  logdir=/var/log/puppet
  rundir=/var/run/puppet
  ca_server=puppet-ca.internal.genfic.com
[master]
  bindaddress="::"
  ca=false</programlisting>

        <para>Make sure no ssl directory is available.</para>

        <programlisting># <command>rm -rf $(puppet master --configprint ssldir)</command></programlisting>

        <para>Next, request a certificate from the CA for this master. In the
        <parameter>--dns_alt_names</parameter> parameter, specify all possible
        hostnames (fully qualified and not) that agents might use to connect
        to this particular master.</para>

        <programlisting># <command>puppet agent --test --dns_alt_names \
    "puppetmaster1.internal.genfic.com,puppet,puppet.internal.genfic.com"</command></programlisting>

        <para>Then, on the CA server, sign the request:</para>

        <programlisting># <command>puppet cert list</command>
# <command>puppet cert sign &lt;new master cert&gt;</command></programlisting>

        <para>Finally, retrieve the signed certificate back on the HUB:</para>

        <programlisting># <command>puppet agent --test</command></programlisting>

        <para>Repeat these steps for every HUB you want to use. You can
        implement round-robin load balancing by using a round-robin DNS
        address allocation for the master hostname (such as
        puppet.internal.genfic.com).</para>
      </section>

      <section>
        <title>Configuring repositories</title>

        <para>As per our initial example, we will need to pull from the
        repositories. Assuming that the git repositories are available at
        git.internal.genfic.com, we could do the following:</para>

        <programlisting># <command>cd /etc/puppet/</command>
# <command>git clone git://git.internal.genfic.com/puppet/manifests.git</command>
# <command>git clone git://git.internal.genfic.com/puppet/modules.git</command>
# <command>git clone git://git.internal.genfic.com/puppet/patterns.git</command></programlisting>

        <para>Of course, you can use nested repositories as well. For
        instance, if you have several administration teams, then inside the
        manifests repository a directory nodes would be available that is
        ignored by git (through the .gitignore file). As a result, anything
        inside that directory is not managed by the manifests.git project. So
        what we do is pull in the repositories of the various teams:</para>

        <programlisting># <command>cd /etc/puppet/manifests/nodes</command>
# <command>git clone git://git.internal.genfic.com/teams/team1.git</command>
# <command>git clone git://git.internal.genfic.com/teams/team2.git</command></programlisting>

        <para>With the environments, a similar setup is used, but once cloned
        we check out a specific branch. For instance, for development
        environment, this would be the development branch:</para>

        <programlisting># <command>cd /etc/puppet/environments</command>
# <command>git clone --branch development git://git.internal.genfic.com/puppet/patterns.git</command></programlisting>

        <para>Because we just pull in changes as they come along, mastership
        of the data is within the git repositories. Given proper policies in
        place, you can easily have a simple script available that is invoked
        by cron to update the various repositories.</para>
      </section>

      <section>
        <title>Adding additional modules</title>

        <para>Puppet supports downloading and installing additional puppet
        modules from the Puppet forge location. To do so, you can use
        <command>puppet module install &lt;name&gt;</command>. For instance,
        to install the inkling/postgresql module:</para>

        <programlisting># <command>puppet module install inkling/postgresql</command>
Preparing to install into /etc/puppet/modules ...
Downloading from http://forge.puppetlabs.com ...
Installing -- do not interrupt ...
/etc/puppet/modules
└─┬ inkling-postgresql (v0.3.0)
  ├── puppetlabs-firewall (v0.0.4)
  └── puppetlabs-stdlib (v3.2.0)</programlisting>

        <para>If you setup environments, you can even have a particular
        version of a module installed inside a specific environment by
        specifying it through <parameter>--environment
        &lt;yourenv&gt;</parameter>.</para>
      </section>
    </section>

    <section>
      <title>Setting up puppet clients</title>

      <para/>

      <section>
        <title>Installing puppet client</title>

        <para>The puppet client, just like the master, is provided by the
        <package>app-admin/puppet</package> package. During the installation,
        portage will also install augeas, which is a tool that abstracts
        configuration syntax and allows simple, automated changes on
        configuration files. Once installed, you can start the puppet client
        service:</para>

        <programlisting># <command>run_init rc-service puppet start</command></programlisting>

        <para>When started, the puppet client will try to connect to the
        server with hostname puppet. If the puppet master is hosted on a
        server with a different hostname, edit the
        <filename>/etc/puppet/puppet.conf</filename> file and add in a
        <parameter>server=</parameter> entry inside the
        <parameter>[agent]</parameter> section. As we are using a
        load-balanced setup, we need to set a dedicated location for the
        certificate handling of the clients - only one server can act as the
        certificate authority. We can point to this server using the ca_server
        parameter:</para>

        <programlisting># <command>cat /etc/puppet/puppet.conf</command>
[main]
  logdir=/var/log/puppet
  rundir=/var/run/puppet
  ssldir=$vardir/ssl

[agent]
  classfile=$vardir/classes.txt
  localconfig=$vardir/localconfig
  listen=true
  ca_server=puppet-ca.internal.genfic.com</programlisting>

        <para>You probably notice we also added the listen=true directive.
        This allows the puppetmaster to connect to the puppet clients as well
        (by default, the puppet clients connect to the master themselves).
        This can be interesting if you want to push changes to particular
        systems without waiting for the standard refresh period.</para>

        <para>Now tell the client to create a certificate and send the signing
        request to the puppet master:</para>

        <programlisting># <command>puppet agent --test</command></programlisting>

        <para>On the puppet master, the certificate request is now pending.
        You can see the list of certificates with puppet cert --list. Sign the
        certificate if you know it is indeed a valid request.</para>

        <programlisting># <command>puppet cert list</command>
  "pg_db1.internal.genfic.com" (23:A5:2F:99:65:60:12:32:00:CA:FE:7F:35:2F:E2:3A)
# <command>puppet cert sign "pg_db1.internal.genfic.com"</command>
notice: Signed certificate request for pg_db1.internal.genfic.com
notice: Removing file Puppet::SSL::CertificateRequest pg_db1.internal.genfic.com at '/var/lib/puppet/ssl/ca/requests/pg_db1.internal.genfic.com.pem'</programlisting>

        <para>Once the request is signed, you can retrieve the certificate
        using the puppet agent command again.</para>

        <programlisting># <command>puppet agent</command></programlisting>
      </section>

      <section>
        <title>Configuring access</title>

        <para>The SELinux policy loaded does not, by default, allow puppet to
        manage each and every file on the system. If you want this, you need
        to enable the <parameter>puppet_manage_all_files</parameter>
        boolean.</para>
      </section>
    </section>

    <section>
      <title>Working with Puppet</title>

      <para/>

      <section>
        <title>Learning the facts</title>

        <para>When you are on a puppet-managed system, you can run
        <command>facter</command><indexterm>
            <primary>facter</primary>
          </indexterm> to get an overview of all the facts that it found on
        the system. For instance, to get information on addresses:</para>

        <programlisting># <command>facter | grep address</command>
ipaddress =&gt; 192.168.100.152
ipaddress6 =&gt; 2001:db8:81:22:0:d8:e8fc:a2dc
macaddress =&gt; 36:5b:94:e1:eb:0e</programlisting>
      </section>

      <section>
        <title>Not using daemon</title>

        <para>If you do not want to use the puppet (client) daemon, you can
        run puppet from cron easily. Just have <command>puppet agent</command>
        ran with the frequency you need.</para>
      </section>

      <section>
        <title>Not using a puppet master</title>

        <para>You can even use Puppet without a puppet master. In that case,
        the local system will need access to the configuration repository
        (which can be a read-only NFS mount or a local checkout of a
        repository).</para>

        <para>In such a situation, you run puppet apply:</para>

        <programlisting># <command>puppet apply --modulepath /path/to/modules /path/to/manifests/site.pp</command></programlisting>

        <para>If you want more information of all changes that are made, you
        can ask puppet to log (using <parameter>-l logfile</parameter>) or
        print it out on screen more (using
        <parameter>--verbose</parameter>).</para>
      </section>

      <section>
        <title>Requesting an immediate check and update</title>

        <para>You can ask the puppet (client) daemon to immediately check with
        the puppet master by sending a SIGUSR1 signal to the daemon
        (restarting the daemon also works).</para>

        <programlisting># <command>pkill -SIGUSR1 puppetd</command></programlisting>

        <para>If your puppet daemons are running with the "listen=true"
        setting, then you can tell the puppet master too to connect to the
        daemon and trigger an immediate check using the "kick" feature:</para>

        <programlisting># <command>puppet kick pg_db1.internal.genfic.com</command></programlisting>
      </section>

      <section>
        <title>Logging</title>

        <para>Puppet logs its activities by default through the system
        logger.</para>

        <programlisting># <command>tail -f /var/log/syslog</command>
puppet-master[4946]: Compiled catalog for puppet.internal.genfic.com in \
  environment production in 0.10 seconds
puppet-agent[6195]: (/Stage[main]/Portage/File[make.conf]/content) content \
  changed '{md5}be84feffe82bc2a37ffc721d892ef06a' to '{md5}5050fba3458f8eb120562db10834e0f1'
puppet-agent[6195]: Finished catalog run in 0.34 seconds</programlisting>
      </section>
    </section>

    <section>
      <title>The power of Puppets definitions</title>

      <para>As I said before, puppets power comes from the ability of
      describing the state of a system, and letting Puppet decide how to reach
      that state. In this section, I give you an overview of how that is
      achieved. Note however that this is far from a crash course on Puppet -
      there are good resources for that online, and Puppet truly warrants an
      entire book on itself.</para>

      <section>
        <title>The site.pp file</title>

        <para>The first file is puppet's
        <filename>manifests/site.pp</filename> file. This file is read by
        Puppet and thus acts as the starting point for all definitions. A
        basic definition for our architecture would be to include the patterns
        and the nodes:</para>

        <programlisting>import "patterns/*.pp"
import "nodes/*.pp"</programlisting>
      </section>

      <section>
        <title>A pattern file</title>

        <note>
          <para>Patterns are specific to our Puppet implementation; the
          terminology is not used in Puppet by itself.</para>
        </note>

        <para>In a pattern file, we declare the settings as if it was a sort
        of "template" node for a particular service. You could have patterns
        for regular systems, for databases, for web application servers, for
        hardened/stripped servers, etc. The idea is that these patterns are
        then applied on the particular nodes, allowing for simple and default
        installations (for instance image-driven) to be quickly transformed
        into proper deployments.</para>

        <para>A pattern described further is just an example for a "general"
        state. You can build on top of this one, build other patterns that
        include from this one, etc. Really, patterns just apply wherever you
        think reuse is good and the setup is somewhat complex (if it was a
        single thing, it would become a module).</para>

        <para>Such a pattern file starts from the next, simple layout (for
        instance for
        <filename>manifests/patterns/general.pp</filename>):</para>

        <programlisting>class general {
  # Include your content here
}</programlisting>

        <para>Let's first include some modules (basic building blocks) -
        something we'll cover later:</para>

        <programlisting># Manage our /etc/hosts file
include hosts
# setup portage settings
include myportage</programlisting>

        <para>Next, we want to make sure that an eix cache (database) is
        available on the system. If not, then Puppet might fail to install
        software itself as it relies on the eix database:</para>

        <programlisting>exec { "create_eix_cache":
  command =&gt; "/usr/bin/eix-update",
  creates =&gt; "/var/cache/eix/portage.eix",
}</programlisting>

        <para>In the above example, Puppet will look at the
        <parameter>creates</parameter> parameter, see that it would create the
        /var/cache/eix/portage.eix file, and then checks on the system if that
        file exists. If not, then the command referenced by the
        <parameter>command</parameter> parameter will be executed by
        Puppet.</para>

        <para>Let's also say that SSH must be installed (on Gentoo this is by
        default, but you never know a wiseguy deleted it) and the service must
        be running. This could be declared as follows:</para>

        <programlisting>package { ssh:
  ensure =&gt; installed,
  require =&gt; Exec['create_eix_cache'],
}

service { sshd:
  ensure =&gt; running,
  require =&gt; [
    File['/etc/ssh/sshd_config'],
    Package[ssh]
  ],
}</programlisting>

        <para>Here, Puppet gets to know that before it can ensure the sshd
        service is running, the /etc/ssh/sshd_config file must be available
        (this does mean we need to declare something later that provides this
        file) and the package as well. And for the package to be installed,
        Puppet knows it first needs to check if our previously created
        eix-command has ran.</para>

        <para>The possibilities are almost endless. You can handle mount
        information too, like so:</para>

        <programlisting>mount { "/usr/portage/packages":
  ensure =&gt; mounted,
  device =&gt; "nfs_server.internal.genfic.com:gentoo/packages",
  atboot =&gt; true,
  fstype =&gt; "nfs4",
  options =&gt; "soft,timeo=30",
}</programlisting>

        <para>With this setting, Puppet will update the
        <filename>/etc/fstab</filename> file and then invoke the mount
        command.</para>
      </section>

      <section>
        <title>A node file</title>

        <para>When a pattern is made, we can create a node (which is the term
        used for a target machine that is managed by Puppet).</para>

        <para>In a simple form, this could be:</para>

        <programlisting>node 'pg1_db.internal.genfic.com' {
  include p_database
}</programlisting>

        <para>Here, we just declared that the node with hostname
        pg1_db.internal.genfic.com uses the pattern definition in
        p_database.pp. Of course, we could extend this further by including
        more information. And the real power comes when the pattern uses
        certain settings (through variables) which are then set on the node
        level.</para>
      </section>

      <section>
        <title>A module file</title>

        <para>Modules are the building blocks of a Puppet-managed environment.
        They provide management features for a single, specific
        technology.</para>

        <para>Say you need to manage a postgresql database, then you can
        search for (and use) a Puppet module for postgresql.</para>

        <programlisting># <command>puppet module search postgresql</command>
Searching http://forge.puppetlabs.com ...
NAME                   DESCRIPTION                            
camptocamp-pgconf      A defined type to manage entries in Postgresql's configuration file
inkling-postgresql     NOTE: Transfer of Ownership
akumria-postgresql     Manage and install Postgresql databases and users
KrisBuytaert-postgres  Puppet Postgres module
puppetlabs-postgresql  Transferred from Inkling

# <command>puppet module install puppetlabs-postgresql</command>
Preparing to install into /etc/puppet/modules ...
Downloading from http://forge.puppetlabs.com ...
Installing -- do not interrupt ...
/etc/puppet/modules
└─┬ puppetlabs-postgresql (v1.0.0)
  ├── puppetlabs-firewall (v0.0.4)
  └── puppetlabs-stdlib (v2.6.0)</programlisting>

        <para>You can manage your own, internal forge site where modules can
        be downloaded from as well.</para>

        <para>When you have a module available, you can start using it similar
        as we did before. On the <link
        xlink:href="http://forge.puppetlabs.com">http://forge.puppetlabs.com</link>
        site you will also find usage examples for each module.</para>
      </section>
    </section>
  </section>

  <section>
    <title>Resources</title>

    <para>A humble list of excellent online resources.</para>

    <para>For git and gitolite:</para>

    <itemizedlist>
      <listitem>
        <para><link
        xlink:href="http://sitaramc.github.com/gitolite/master-toc.html">gitolite
        documentation</link> (github.com)</para>
      </listitem>
    </itemizedlist>

    <para>For puppet:</para>

    <itemizedlist>
      <listitem>
        <para><link xlink:href="http://www.puppetcookbook.com/">Puppet
        Cookbook</link> (puppetcookbook.com)</para>
      </listitem>
    </itemizedlist>
  </section>
</chapter>
