<?xml version="1.0" encoding="UTF-8"?>
<chapter version="5.0" xml:id="fileserver"
         xmlns="http://docbook.org/ns/docbook"
         xmlns:xlink="http://www.w3.org/1999/xlink"
         xmlns:xi="http://www.w3.org/2001/XInclude"
         xmlns:svg="http://www.w3.org/2000/svg"
         xmlns:m="http://www.w3.org/1998/Math/MathML"
         xmlns:html="http://www.w3.org/1999/xhtml"
         xmlns:db="http://docbook.org/ns/docbook">
  <title>High Available File Server</title>

  <section>
    <title>Introduction</title>

    <para>For some applications or services, a high available file server is
    necessary. Although clustering file systems exist (like OCFS and GFS), in
    this chapter, the focus lies on providing a high-available, secure file
    server based on NFS v4.</para>

    <para>Users that are interested in providing network file systems across a
    heterogeneous network of Unix, Linux and Windows systems might want to
    read about Samba instead.</para>

    <para>In another chapter, distributed storage is presented as a different
    approach to high available storage. </para>
  </section>

  <section>
    <title>File server requirements</title>

    <para>A file server is responsible for managing large sets of unstructured
    data. Important requirements for it are therefore</para>

    <itemizedlist>
      <listitem>
        <para>high availability (little to no downtime)</para>
      </listitem>

      <listitem>
        <para>good performance</para>
      </listitem>

      <listitem>
        <para>well managed backup/restore routines</para>
      </listitem>

      <listitem>
        <para>access control on the files and directories</para>
      </listitem>

      <listitem>
        <para>good capacity management</para>
      </listitem>
    </itemizedlist>

    <para>Optionally, the following services might be of interest as
    well:</para>

    <itemizedlist>
      <listitem>
        <para>anti-malware scanning in case clients are using
        malware-sensitive operating systems or designs</para>
      </listitem>

      <listitem>
        <para>file retention support</para>
      </listitem>

      <listitem>
        <para>tiered data storage</para>
      </listitem>
    </itemizedlist>

    <section>
      <title>Hard requirements</title>

      <para/>

      <section>
        <title>High availability</title>

        <para>The file system must be (very) high available. Multiple services
        should be positioned that can take over serving file requests if one
        goes down. A fail-over of the service will incur a small service
        disruption though.</para>
      </section>

      <section>
        <title>Good performance</title>

        <para>All file operations must return within reasonable
        performance.</para>
      </section>

      <section>
        <title>Backup/restore</title>

        <para>Backup and restore operations must be possible on the file
        server level without disruption of the file service itself, and
        sufficiently granular. Most of the time, users will request a restore
        of a single file or directory anyway.</para>
      </section>

      <section>
        <title>Access control</title>

        <para>Make sure access to file shares is properly governed. This
        includes both host access (which system is allowed to access mounts)
        as well as individual user rights.</para>
      </section>

      <section>
        <title>Capacity management</title>

        <para>The capacity used by the file server should be very well
        monitored. The necessary thresholds should be put in place so that
        additional storage can be purchased before the file system is full, as
        well as other resources (like CPU resources in case the file system
        becomes overloaded).</para>
      </section>
    </section>

    <section>
      <title>Optional features</title>

      <para/>

      <section>
        <title>Anti-malware scanning</title>

        <para>With anti-malware scanning on the file server during off-hours
        potential threats can be removed before they do harm. Of course,
        on-access file scanning through the clients is still
        recommended.</para>
      </section>

      <section>
        <title>File retention support</title>

        <para>Although probably handled through organizational processes
        rather than technology, having file retention on the file server can
        help to keep the amount of storage consumption under control. </para>
      </section>

      <section>
        <title>Tiered data storage</title>

        <para>If data access times are kept, one can consider moving less
        often used data towards slower data storage. This keeps the cost of
        the infrastructure lower while providing good performance.</para>
      </section>
    </section>
  </section>

  <section>
    <title>Architecture</title>

    <para>The used architecture for a high-available NFS service is
    simple:</para>

    <figure>
      <title>NFS architecture</title>

      <mediaobject>
        <imageobject>
          <imagedata fileref="images/07-nfs.png"/>
        </imageobject>
      </mediaobject>
    </figure>

    <para>In the drawing, general components such as configuration management,
    backup &amp; monitoring are left out.</para>

    <para>The OpenAIS service is used for messaging state changes. Changes are
    communicated to Pacemaker, a cluster management framework, which is
    responsible for automatically updating services based on the input.</para>

    <section>
      <title>Flows and feeds</title>

      <para>The most obvious (and important) flow is the DRBD sync. Important
      to know here is that the synchronization is handled by a kernel
      subsystem, not a userspace daemon.</para>
    </section>

    <section>
      <title>Administration</title>

      <para/>

      <section>
        <title>DRBD administration</title>

        <para/>

        <figure>
          <title>DRBD Administration</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="images/07-drbd-admin.png"/>
            </imageobject>
          </mediaobject>
        </figure>
      </section>

      <section>
        <title>OpenAIS administration</title>

        <para/>
      </section>

      <section>
        <title>Pacemaker administration</title>

        <para/>
      </section>
    </section>

    <section>
      <title>Monitoring</title>

      <para/>
    </section>

    <section>
      <title>Operations</title>

      <para/>
    </section>

    <section>
      <title>Users</title>

      <para/>
    </section>

    <section>
      <title>Security</title>

      <para/>
    </section>
  </section>

  <section>
    <title>NFS v4</title>

    <para>NFS stands for <emphasis>Network File System</emphasis> and is a
    well-known distributed file system in Unix/Linux. NFS (and its versions,
    like NFSv2, NFSv3 and NFSv4) are open standards and defined in various
    RFCs. It provides an easy, integrated way for sharing files between
    Unix/Linux systems as systems that have access to NFS mounts see them as
    part of the (local) file system tree. In other words,
    <filename>/home</filename> can very well be an NFS-provided, remote mount
    rather than a locally mounted file system.</para>

    <section>
      <title>Architecture</title>

      <para>NFS at first might seem a bit difficult to master. In the past
      (NFSv3 and earlier), this might have been true, but with NFSv4, its
      architecture has become greatly simplified, especially when using TCP
      rather than UDP.</para>

      <figure>
        <title>NFSv3 versus NFSv4</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="images/05-nfs.png" scalefit="0" width="100%"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>With NFS v3, next to the NFS daemon (<command>nfsd</command>), the
      following services need to run as well:</para>

      <itemizedlist>
        <listitem>
          <para><command>rpcbind</command><indexterm>
              <primary>rpcbind</primary>
            </indexterm>, which is responsible for dynamically assigning ports
          for RPC services. The NFS server tells the
          <command>rpcbind</command> daemon on which address it is listening
          and which RPC program numbers it is prepared to serve. A client
          connects to the <command>rpcbind</command> daemon and informs it
          about the RPC program number the client wants to connect with. The
          <command>rpcbind</command> daemon then replies with the address that
          NFS listens on.</para>

          <para>It is the <command>rpcbind</command> daemon which is
          responsible for handling access control (through the tcp wrappers
          interface - using the <filename>/etc/hosts.allow</filename> and
          <filename>hosts.deny</filename> files) - for the service "rpcbind".
          Note though that this access control is then applied to all
          RPC-enabled services, not only NFS.</para>
        </listitem>

        <listitem>
          <para><command>rpc.mountd</command><indexterm>
              <primary>rpc.mountd</primary>
            </indexterm> implements the NFS mount protocol and exposes itself
          as an RPC service as well. A client issues a MOUNT request to the
          <command>rpc.mountd</command> daemon, which checks its list of
          exported file systems and access controls to see if the client can
          mount the requested directory. Access controls can both be mentioned
          within the list of exported file systems as well as through
          <command>rpc.mountd</command>' tcp wrappers support (for the service
          "mountd").</para>
        </listitem>

        <listitem>
          <para><command>rpc.lockd</command><indexterm>
              <primary>rpc.lockd</primary>
            </indexterm> supports file locking on NFS, so that concurrent
          access to the same resource can be streamlined.</para>
        </listitem>

        <listitem>
          <para><command>rpc.statd</command><indexterm>
              <primary>rpc.statd</primary>
            </indexterm> interacts with the <command>rpc.lockd</command>
          daemon and provides crash and recovery for the locking
          services.</para>
        </listitem>
      </itemizedlist>

      <para>With NFSv4, mount and locking services have been integrated in the
      NFS daemon itself. The <command>rpc.mountd</command> daemon is still
      needed to handle the exports, but is not involved with network
      communication anymore (in other words, the client connects directly with
      the NFS daemon).</para>

      <para>Although the high-level architecture is greatly simplified (and
      especially for the NFS client, since all accesses are now done through
      the NFS daemon), other daemons are being introduced to further enhance
      the functionalities offered by NFSv4.</para>

      <section>
        <title>ID Mapping</title>

        <para>The first daemon is <command>rpc.idmapd</command><indexterm>
            <primary>rpc.idmapd</primary>
          </indexterm>, which is responsible for translating user and group
        IDs to names and vice-versa. It runs on both the server and client so
        that communication between the two can use user names (rather than
        possibly mismatching user IDs).</para>
      </section>

      <section>
        <title>Kerberos support</title>

        <para>A second daemon (well, actually set of daemons) is
        <command>rpc.gssd</command><indexterm>
            <primary>rpc.gssd</primary>
          </indexterm> (client) and <command>rpc.svcgssd</command><indexterm>
            <primary>rpc.svcgssd</primary>
          </indexterm> (server). These daemons are responsible for handling
        the kerberos authentication mechanism for NFS if Kerberos is
        used.</para>
      </section>
    </section>

    <section>
      <title>Installation</title>

      <para>On both server as well as clients install the necessary supporting
      tools (and perhaps kernel settings).</para>

      <section>
        <title>Server-side installation</title>

        <para>On the server, first make sure that the following kernel
        configuration settings are at least met:</para>

        <programlisting>CONFIG_NFS_FS=y
CONFIG_NFSD_V4=y
CONFIG_NFSD_V4_ACL=y</programlisting>

        <para>Next install the NFS server (which is provided through the
        <package>nfs-utils</package> package) while making sure that USE="ipv6
        nfsv4 nfsv41" are at least set</para>

        <programlisting># <command>emerge nfs-utils</command></programlisting>

        <para>Edit <filename>/etc/conf.d/nfs</filename> and set the
        OPTS_RPC_MOUNTD and OPTS_RPC_NFSD parameters to enable NFSv4 and
        disable NFSv3 and NFSv2 support:</para>

        <programlisting>OPTS_RPC_MOUNTD="-V 4 -N 3 -N 2"
OPTS_RPC_NFSD="-N 2 -N 3"</programlisting>

        <para>The following directories might need to be created as well if
        the SELinux doesn't allow the init scripts to create those:</para>

        <programlisting># <command>mkdir /var/lib/nfs/{v4root,v4recovery,rpc_pipefs}</command></programlisting>

        <para>Edit <filename>/etc/idmapd.conf</filename> and set the domain
        name (this setting should be the same on client and server).</para>

        <programlisting># <command>vim /etc/idmapd.conf</command>
[General]
Domain = internal.genfic.com</programlisting>

        <para>Now start the NFS service.</para>

        <programlisting># <command>run_init rc-service nfs start</command></programlisting>
      </section>

      <section>
        <title>Client-side installation</title>

        <para>On the clients, make sure that the following kernel
        configuration settings are at least met:</para>

        <programlisting>CONFIG_NFS_FS=y
CONFIG_NFS_V4=y
CONFIG_NFS_V4_ACL=y</programlisting>

        <para>Next, install the NFS utilities while making sure that USE="ipv6
        nfsv4 nfsv41" are at least set:</para>

        <programlisting># <command>emerge nfs-utils</command></programlisting>

        <para>Edit /etc/idmapd.conf and set the domain name (this setting
        should be the same on client and server).</para>

        <programlisting># <command>vim /etc/idmapd.conf</command>
[General]
Domain = internal.genfic.com</programlisting>

        <para>Finally, start the <command>rpc.statd</command> and
        <command>rpc.idmapd</command> daemons:</para>

        <programlisting># <command>run_init rc-service rpc.statd start</command>
# <command>run_init rc-service rpc.idmapd start</command></programlisting>
      </section>
    </section>

    <section>
      <title>Configuration</title>

      <para>Most NFS configuration is handled through the
      <filename>/etc/exports</filename><indexterm>
          <primary>exports</primary>
        </indexterm> file.</para>

      <section>
        <title>Shared file systems in /etc/exports</title>

        <para>To share file systems (or file structures) through NFS, edit the
        <filename>/etc/exports</filename> file. Below is a first example for
        an exported packages directory:</para>

        <programlisting>/srv/nfs                        2001:db8:81:e2:0:26b5:365b:5072/48(rw,fsid=0,no_subtree_check,no_root_squash,sync)
/srv/nfs/gentoo/packages        2001:db8:81:e2:0:26b5:365b:5072/48(rw,sync,no_subtree_check,no_root_squash)</programlisting>

        <para>The first one contains the <parameter>fsid=0</parameter>
        parameter. This one is important, as it tells NFSv4 what the root is
        of the file systems that it exports. In this case, that is
        <filename>/srv/nfs</filename>. A client will then mount file systems
        relative to this root location (so mount
        <filename>gentoo/packages</filename>, not
        <filename>srv/nfs/gentoo/packages</filename>).</para>

        <para>When NFS mounts are failing, make sure to
        double/triple/quadruple-check the exports file. NFS is not good at
        logging things, but in the majority of cases, the problem lies with a
        misconfiguration in the <filename>exports</filename> file.</para>
      </section>
    </section>
  </section>

  <section>
    <title>Synchronous or asynchronous replication with DRBD</title>

    <para>In some organizations, the requirements might be so high that the
    organization doesn't want any data loss. If this is the case, DRBD must be
    configured to use synchronous replication. This means that data writes are
    only acknowledged to the client if it has been replicated as well.
    Synchronous replication incurs a small performance penalty though, so it
    might make sense to offer file services for both synchronous and
    asynchronous replication.</para>

    <section>
      <title>Installation</title>

      <para>First, install drbd and drbd-kernel.</para>

      <programlisting># <command>emerge drbd drbd-kernel</command></programlisting>

      <para>So far for the easy stuff ;-)</para>
    </section>

    <section>
      <title>Configuring DRBD</title>

      <para>Now first configure the DRBD replication. In
      <filename>/etc/drbd.d/nfs-disk.res</filename> (on both systems - the
      file name is completely free to choose, but keep the
      <filename>.res</filename> extension), configure the replication as
      such:</para>

      <programlisting>global {
  usage-count yes;
}

common {
  syncer { rate 100M; }
}

resource r0 {
  protocol C;
  handlers {
    pri-on-incon-degr "echo o &gt; /proc/sysrq-trigger ; halt -f";
    pri-lost-after-sb "echo o &gt; /proc/sysrq-trigger ; halt -f";
    local-io-error "echo o &gt; /proc/sysrq-trigger ; halt -f";
  }

  startup {
    degr-wfc-timeout 120;
  }

  disk {
    on-io-error   detach;
  }

  net {
  }

  syncer {
    rate 100M;
    al-extents 257;
  }

on nfs_p {
    device /dev/drbd0;
    disk /dev/sdb1;
    address nfs_p:7788;
    meta-disk /dev/sdc1[0];
  }

on nfs_s {
    device /dev/drbd0;
    disk /dev/sdb1;
    address nfs_s:7788;
    meta-disk /dev/sdc1[0];
  }
}</programlisting>

      <para>In this example, the replication resource (r0) is defined and uses
      protocol C (which, in DRBD's terms, means synchronous replication - use
      A if asynchronous replication is wanted). On the hosts (with hostnames
      nfs_p for the primary, and nfs_s for the secondary) the
      <filename>/dev/sdb1</filename> partition is managed by DRBD, and
      <filename>/dev/sdc1</filename> is used to store the DRBD
      meta-data.</para>

      <para>Next, create the managed device on both systems:</para>

      <programlisting># <command>drbdadm create-md r0</command>
# <command>drbdadm up all</command></programlisting>

      <para>The state of the DRBD setup can be followed up on in
      <filename>/proc/drbd</filename>. At this point, on both systems it
      should show their resources be in secondary mode, so enable it on one
      side:</para>

      <programlisting># <command>drbdsetup /dev/drbd0 primary -o</command></programlisting>

      <para>Now the drbd0 device is ready to be used (create file systems on
      it and mount it on systems).</para>
    </section>

    <section>
      <title>Start the services</title>

      <para>Start the installed services to get it up and running.</para>

      <programlisting># <command>rc-update add drbd default</command>
# <command>rc-service drbd start</command></programlisting>
    </section>

    <section>
      <title>Managing DRBD</title>

      <para>Run <command>drbd-overview</command> to get a view on the
      replication state of the system:</para>

      <programlisting># <command>drbd-overview</command>
0:nfs-disk             Connected Primary/Secondary
  UpToDate/UpToDate C r--- /srv/data ext4 150G  97G  52G  65%</programlisting>

      <para>Another file to keep an eye on is
      <filename>/proc/drbd</filename>:</para>

      <programlisting># <command>cat /proc/drbd</command>
 0: cs:Connected ro:Primary/Secondary ds:UpToDate/UpToDate C r-----
    ns:0 nr:0 dw:0 dr:656 al:0 bm:0 lo:0 pe:0 ua:0 ap:0 ep:1 wo:b oos:0</programlisting>

      <para>This particular example can be read as "Resource 0 has its
      connection state (cs) set as Connected. The role (ro) of the disk is
      Primary, its partner disk has the role Secondary. The disk states (ds)
      are both UpToDate and the synchronisation protocol is C
      (synchronous)."</para>

      <para>The "r-----" shows the I/O flags of the resource (more details in
      the DRBD user guide) whereas the next line gives a general overview of
      the various performance indicators.</para>

      <para>If the disks need to be brought down manually, use the drbdadm
      command:</para>

      <programlisting># <command>drbdadm down nfs-disk</command></programlisting>
    </section>
  </section>

  <section>
    <title>OpenAIS</title>

    <para/>

    <section>
      <title>Configuration</title>

      <para>First install OpenAIS:</para>

      <programlisting># <command>emerge openais</command></programlisting>

      <para>The OpenAIS configuration is handled through the
      <filename>/etc/ais/openais.conf</filename> file:</para>

      <programlisting>aisexec {
        user:   root
        group:  root
}
service {
        name: pacemaker
        ver:  0
}
totem {
        version: 2
        token:          1000
        hold: 180
        token_retransmits_before_loss_const: 20
        join:           60
        consensus:      4800
        vsftype:        none
        max_messages:   20
        clear_node_high_bit: yes
        secauth: off
        threads: 0
        interface {
                ringnumber: 0
                bindnetaddr: 192.168.10.0
                mcastaddr: 226.94.1.2
                mcastport: 5406
        }
}
logging {
        debug: off
        fileline: off
        to_syslog: yes
        to_stderr: no
        syslog_facility: daemon
        timestamp: on
}
amf {
        mode: disabled
}</programlisting>

      <para>Then start the OpenAIS daemon:</para>

      <programlisting># <command>rc-update add openais default</command>
# <command>rc-service openais start</command></programlisting>
    </section>
  </section>

  <section>
    <title>Pacemaker</title>

    <para>Pacemaker is the cluster software that administrators will work with
    the most. It receives feedback from the OpenAIS messaging framework to
    know about the state of all resources.</para>

    <section>
      <title>Configuring pacemaker</title>

      <para>First install pacemaker:</para>

      <programlisting># <command>emerge pacemaker</command></programlisting>

      <para>The cluster can be configured using the <command>crm</command>
      command. In the next example, quorum support is disabled (in a two-node
      cluster, quorums are not concerned, although the use of a quorum is
      recommended to properly handle communication failures between nodes) and
      resource stickiness is enabled to keep resources where they are after a
      fail-over:</para>

      <programlisting># <command>crm</command>
crm(live)# <command>configure</command>
crm(live)configure# <command>property no-quorum-policy=ignore</command>
crm(live)configure# <command>property default-resource-stickiness=1000</command>
crm(live)configure# <command>commit</command>
crm(live)configure# <command>bye</command></programlisting>
    </section>
  </section>

  <section>
    <title>Heartbeat</title>

    <para>Heartbeat is a process that regularly checks the state of a service
    (or set of services) and performs predefined actions when a service is
    unavailable. It is a simple yet powerful method for creating
    high-available setups.</para>

    <section>
      <title>Configuring heartbeat</title>

      <para>First install heartbeat:</para>

      <programlisting># <command>emerge heartbeat</command></programlisting>

      <para>Next configure heartbeat. There are three files that need some
      attention: the first one is <filename>/etc/ha.d/ha.cf</filename>, the
      second one is an authorization key used to authenticate and authorize
      requests (so that a third party cannot influence the heartbeat) and the
      last one identifies the resource(s) itself.</para>

      <para>So first the <filename>ha.cf</filename> file:</para>

      <programlisting>logfacility local0
keepalive 1
deadtime 10
bcast eth1
auto_failback on
node nfs_p nfs_s</programlisting>

      <para>In <filename>/etc/heartbeat/authkeys</filename>, put:</para>

      <programlisting>auth 3
3 sha1 TopS3cr1tPassword</programlisting>

      <para>In <filename>/etc/ha.d/haresources</filename> put:</para>

      <programlisting>nfs_p IPaddr::nfs_p
nfs_p drbddisk::r0 Filesystem::/dev/drbd0::/data::ext3 nfs-kernel-server</programlisting>

      <para>On the secondary server, use the same but use nfs_s. This will
      ensure that, once the secondary server becomes the primary (a fail-over
      occurred) this remains so, even when the primary system is back up. If a
      switch-over should occur when the primary gets back, use the same file
      content on both systems.</para>
    </section>

    <section>
      <title>Start the services</title>

      <para>Start the installed services to get the heartbeat up and
      running.</para>

      <programlisting># <command>rc-update add heartbeat default</command>
# <command>rc-service heartbeat start</command></programlisting>
    </section>

    <section>
      <title>Split brain</title>

      <para>A split brain is something to avoid, but need to be trained on. A
      split brain occurs when both sides of the synchronisation believe they
      are the primary disk (i.e. the other side has failed and, while the
      primary disk remains active, the secondary disk invoked a fail-over and
      became active). A DRBD split brain will be detected by DRBD itself when
      both resources can "see" each other again. When this occurs, the
      replication is immediately stopped, one disk will be in StandAlone
      connection state while the other will be in either StandAlone or in
      WFConnection (WF = Waiting For).</para>

      <para>Although it is possible to configure DRBD to automatically resolve
      a split brain, it is important to understand that this always involves
      sacrificing one of the nodes: changes made on the node that gets
      sacrificed will be lost. In the NFS case, it is necessary to check where
      heartbeat had the NFS service running. If the NFS service remained
      running on the primary node (nfs_p) then sacrifice the secondary
      one.</para>

      <programlisting>$ <command>ssh nfs_p cat /proc/drbd</command>
0: cs:Standalone st:Primary/Secondary ds:UpToDate/Unknown C r---
$ <command>ssh nfs_s cat /proc/drbd</command>
0: cs:WFConnection:Primary/Secondary ds:UpToDate/Unknown C r---</programlisting>

      <para>Assuming that heartbeat left the NFS service running on the
      primary node, sacrifice the secondary and reset the
      synchronisation.</para>

      <programlisting>$ <command>ssh nfs_s sudo rc-service heartbeat stop</command>
* Stopping heartbeat...     [ ok ]
$ <command>ssh nfs_p sudo rc-service heartbeat stop</command>
* Stopping heartbeat...     [ ok ]</programlisting>

      <para>Now log on to the secondary node (nfs_s), explicitly mark it as a
      secondary one, reset its state and reconnect:</para>

      <programlisting># <command>drbdadm secondary nfs-disk</command>
# <command>drbdadm disconnect nfs-disk</command>
# <command>drbdadm -- --discard-my-data connect nfs-disk</command></programlisting>

      <para>Now log on to the primary node (nfs_p) and connect the resource
      again.</para>

      <programlisting># <command>drbdadm connect nfs-disk</command></programlisting>

      <para>Restart the heartbeat service and everything should be set up
      again.</para>
    </section>
  </section>

  <section>
    <title>Performance tuning</title>

    <para/>

    <section>
      <title>Tuning file system performance - dir_index</title>

      <para>First of all, tune the file system performance. One of the
      optimizations to consider is to use <parameter>dir_index</parameter>
      enabled file systems.</para>

      <para>When the file system (ext3 or ext4) uses
      <parameter>dir_index</parameter> (ext4 uses this by default), lookup
      operations on the file system uses hashed b-trees to speed up the
      operation. This algorithm has a huge influence on the time needed to
      list directory content. A quick (overly simplified and thus also
      incorrect) view at the approach is given next.</para>

      <figure>
        <title>HTree in a simple example</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="images/05-htree.png" scalefit="0" width="100%"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>In the above drawing, the left side shows a regular, linear
      structure: the top block represents the directory, which holds the
      references (numbers) of the files. The files themselves are stored in
      the blocks with the capital letter (which includes metadata of the file,
      such as modification time - which is used by rsync to find the files it
      needs to replicate). The small letters contain some information about
      the file (which is for instance the file name). In the right side, a
      possible example of an HTree (hashed b-tree) is shown.</para>

      <para>So how does this work? Let's assume a process needs to traverse
      all files (for instance <command>rsync</command>) and check the
      modification times. How many "steps" would this take?</para>

      <itemizedlist>
        <listitem>
          <para>In the left example (ext2), the code first gets the overview
          of file identifications. It starts at the beginning of the top
          block, finds one reference to a file (1), then after it looks where
          the information about the second file is at. Because the name of a
          file ("a" in this case) is not of a fixed width, the reference to
          the file (1) also contains the offset where the next file is at. So
          the code jumps to the second file (2), and so on. At the end, the
          code returns "1, 2, 5, 7, 8, 12, 19 and 21" and has done 7 jumps to
          do so (from 1 to 2, from 2 to 5, ...)</para>

          <para>Next, the code wants to get the modification time of each of
          those files. So for the first file (1) it looks at its address and
          goes to the file. Then, it wants to do the same for the second file,
          but since it only has the number (2) and not the reference to the
          file, it repeats the process: read (1), jump, read (2) and look at
          file. To get the information about the last file, it does a sequence
          similar to "read 1, jump, read 2, jump, read 5, jump, ..., read 21,
          look at file". At the end, this took 36 jumps. In total, the code
          used 43 jumps.</para>
        </listitem>

        <listitem>
          <para>In the right example (using HTrees), the code gets the
          references first. The list of references (similar to the numbers in
          the left code) use fixed structures so within a node, no jumps are
          needed. So starting from the top node [c;f] it takes 3 jumps (one to
          [a;b], one to [d;e] and one to [g;h]) to get an overview of all
          references.</para>

          <para>Next, the code again wants to get the modification time of
          each of those files. So for the first file (A) it reads the top
          node, finds that "a" is less than "c" so goes to the first node
          lower. There, it finds the reference to A. So for file A it took 2
          jumps. Same for file B. File c is referred in the top node, so only
          takes one jump, etc. At the end, this took 14 jumps, or in total 17
          jumps.</para>
        </listitem>
      </itemizedlist>

      <para>So even for this very simple example (with just 8 files) the
      difference is 43 jumps (random-ish reads on the disk) versus 17 jumps.
      This is, what in algorithm terms, is O(n^2) versus O(nlog(n)) speed: the
      "order" of the first example goes with n^2 whereas the second one is
      n.log(n) - "order" here means that the impact (number of jumps in our
      case) goes up the same way as the given formula - it doesn't give the
      exact number of jumps. For very large sets of files, this gives a very
      huge difference (100^2 = 10'000 whereas 100*log(100) = 200, and 1000^2 =
      1'000'000 whereas 1000*log(1000) = 3000).</para>

      <para>Hence the speed difference.</para>

      <para>The <parameter>dir_index</parameter> parameter can be set using
      <command>tune2fs</command>:</para>

      <programlisting># <command>tune2fs -O dir_index /dev/vda1</command>
# <command>umount /dev/vda1</command>
# <command>fsck -Df /dev/vda1</command></programlisting>
    </section>
  </section>

  <section>
    <title>Resources</title>

    <para>NFS</para>

    <itemizedlist>
      <listitem>
        <para><link xlink:href="https://tools.ietf.org/html/rfc3530">Network
        File System version 4 protocol</link> (RFC 3530 on IETF.org)</para>
      </listitem>

      <listitem>
        <para><link xlink:href="https://tools.ietf.org/html/rfc5661">Network
        File System version 4.1 protocol</link> (RFC 5661 on IETF.org)</para>
      </listitem>
    </itemizedlist>

    <para>DRBD</para>

    <itemizedlist>
      <listitem>
        <para><link xlink:href="http://www.drbd.org/users-guide/">DRBD Users
        Guide</link> (on drbd.org)</para>
      </listitem>
    </itemizedlist>
  </section>
</chapter>
